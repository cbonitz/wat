\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\Why}[1]{\textsf{Why}(#1)}

\section{Background}

\subsection{Causality}
Time is fundamental to our understanding of how events are ordered. It's clear,
tautological even, that if an event occurs at 6:42, then it \emph{happens
before} another event that occurs at 6:45. Unfortunately, accurately measuring
time in a distributed system is infeasible~\cite{marzullo1984maintaining,
sampath2012synchronization, schmid2000orthogonal}. Clocks on different servers
within a distributed system drift apart, so servers cannot agree on a single
global notion of time, and thus they cannot agree on a single global
\emph{total order} of events. However, as Lamport showed in
\cite{lamport1978time}, it \emph{is} possible for servers to agree on a global
\emph{partial order} of events that respects the global passage of time. This
partial ordering of events also dictates which events can causally affect each
other.

To make this partial ordering and notion of causality precise, we consider a
set of single-threaded servers that communicate over the network. Every server
$a$ serially executes a sequence of events $a_1, a_2, a_3, \ldots$ where each
event $a_i$ represents the action of server $a$ either (1) performing local
computation, (2) sending a message to another server, or (3) receiving a
message from another server. For example, consider the execution illustrated in
\figref{Causality}. There are three servers ($a$, $b$, and $c$) and eight
events ($a_1, a_2, \ldots, c_2$).  $a_1$ represents a local computation on $a$,
$a_2$ represents server $a$ sending a message to server $b$, and event $b_2$
represents the receipt of this message by server $b$.

{\input{figures/causality}}

We define the \defword{happens before} relation $\happensbefore$ on events as
the smallest transitive relation such that
%
(1)
  if $a_i, a_j$ are two events within the same process (e.g. $a_1, a_2$ in
  \figref{Causality}), then $a_i$ happens before $a_j$, and
(2)
  if $a_i$ and $b_j$ are the sending and receiving of a message respectively
  (e.g. $a_2$ and $b_2$ in \figref{Causality}), then $a_i$ happens before
  $b_j$.
%
For example, in \figref{Causality}, $b_1 \happensbefore b_2$, and $a_1
\happensbefore c_2$. The happens before relation is a partial order and
formalizes our intuitive notion of which events can causally affect each other.
An event $a_i$ can only be caused by an event $b_j$ that happens before it.
The set $\setst{b_j}{b_j \happensbefore a_i}$ is called the \defword{causal
history} of $a_i$.

\subsection{Data Provenance}
Given a relational database instance $I$, a relational algebra query $Q$, and a
tuple $t$ in the output of the query, it's natural to ask why $t$ appears in
the output. For example, consider the relational database instance given in
\figref{DataProvenance} which describes owners, their pets, and the things that
their pets like. And, consider the relation algebra query
\[
  Q \defeq \pi_{\ttt{owner}}(
             \sigma_{\ttt{likes} = \text{sunlight}}(
               \ttt{Owners} \join \ttt{Pets}
             )
           )
\]
that returns the name of all owners who own a pet that likes sunlight.
Evaluating $Q$ on $I$ produces the tuple $t = \text{(Ava)}$.

{\input{figures/data_provenance.tex}}

Intuitively, $t$ is present in the output $Q(I)$ for two reasons: (1) the
existence of the (Ava, Mimi) and (Mimi, sunlight) tuples and (2) the existence
of the (Mimi, Harry) and (Harry, sunlight) tuples.
\defword{\Whyprovenance{}}~\cite{buneman2001and, cheney2009provenance} is a
form of data provenance that formalizes this intuition. The
\whyprovenance{}\footnote{
  For a formal definition of \whyprovenance{}, we refer the reader to
  \cite{cheney2009provenance}. For our purposes, an informal understanding of
  \whyprovenance{} is sufficent.
}
of a tuple $t$ with respect to query $Q$ and database instance $I$, denoted
$\Why{Q, I, t}$, is a set $J_1, \ldots, J_n$ of subinstances where each
subinstance $J_i \subseteq I$ suffices to produce $t$ (i.e.\ $t \in Q(J_i)$).
%
Returning to our example above, the \whyprovenance{} of the (Ava) tuple is the
set $\set{J_1, J_2}$ where
\begin{align*}
  J_1 &= \set{(\text{Ava}, \text{Mimi}), (\text{Mimi}, \text{sunlight})} \\
  J_2 &= \set{(\text{Ava}, \text{Harry}), (\text{Harry}, \text{sunlight})}
\end{align*}

% The definition of \Whyprovenance{} for select ($\sigma$), project ($\pi$), join
% ($\join$), union ($\cup$) queries taken from \cite{cheney2009provenance} is as
% follows:
% \begin{align*}
%   \Why{\set{t}, I, u} &= \begin{cases}
%     \set{\emptyset}, & \text{if $(t = u)$,} \\
%     \emptyset, & \text{otherwise.} \\
%   \end{cases} \\
%   \Why{R, I, t} &= \begin{cases}
%     \set{\set{t}}, & \text{if $t \in R(I)$,} \\
%     \emptyset, & \text{otherwise.} \\
%   \end{cases} \\
%   \Why{\sigma_\theta(Q), I, t} &= \begin{cases}
%     \Why{Q, I, t}, & \text{if $\theta(t)$,} \\
%     \emptyset, & \text{otherwise.} \\
%   \end{cases} \\
%   \Why{\pi_U(Q), I, t} &= \bigcup_{\setst{u \in Q(I)}{t = u[U]}} \Why{Q, I, u} \\
%   \Why{Q_1 \join Q_2} &= \Why{Q_1, I, t} \Cup \Why{Q_2, I, t} \\
%   \Why{Q_1 \cup Q_2} &= \Why{Q_1, I, t} \cup \Why{Q_2, I, t} \\
% \end{align*}

\subsection{State Machines}
It's common to model servers---like FTP servers or key-value stores---as
deterministic state machines that repeatedly receive requests, update their
state, and send replies~\cite{schneider1990implementing, lamport1998part}. More
precisely, a \defword{deterministic state machine} $M = (S, s_0, \Sigma,
\Lambda, \delta, \epsilon)$ consists of
  a (potentially infinite) set $S$ of states,
  a start state $s_0 \in S$,
  an input alphabet $\Sigma$,
  an output alphabet $\Lambda$,
  a transition function $\delta: S \times \Sigma \to S$, and
  an output function $\epsilon: S \times \Sigma \to \Lambda$.
A state machine $M$ begins in state $s_0$ and repeatedly receives inputs $a \in
\Sigma$. Upon receiving an input $a$, $M$ transitions from state $s$ to state
$\delta(s, a)$ and outputs $\epsilon(s, a)$.

We refer to a sequence of inputs received by a state machine as a
\defword{trace} $T = a_1 a_2 \ldots a_n \in \Sigma^*$. A \defword{subtrace}
$T'$ of $T$ is a subsequence $T' = a_{i_1} a_{i_2} \ldots a_{i_m}$ where $i_1,
i_2, \ldots, i_m$ are distinct elements of $1, 2, \ldots, n$ in ascending
order. Note that a subsequence does \emph{not} have to be contiguous. For
example, $T' = a_1 a_3$ is a subtrace of $T = a_1 a_2 a_3 a_4$. If $T'$ is a
subtrace of $T$, then a \defword{supertrace of $T'$ in $T$} is a subtrace of
$T$ that contains every element of $T'$. An illustrative example of the
definition of subtraces and supertraces is given in \figref{Traces}.

{\input{figures/traces.tex}}

$\delta$ takes in a state $s \in \Sigma$ and a single input $a \in \Sigma$.
It's convenient to extend $\delta$ to a function $\delta^*: S \times \Sigma^*
\to S$ that takes in a state $s$ and a trace $T \in \Sigma^*$ and outputs the
state reached after sequentially executing the inputs in $T$ starting in state
$s$. Similarly, we can extend $\epsilon$ to a function $\epsilon: S \times
\Sigma^+ \to \Lambda$ which takes in a state $s$ and a non-empty trace $T = a_1
\ldots a_n \in \Sigma^+$ and returns $\epsilon(\delta^*(s, a_1 a_2 \ldots
a_{n-1}), a_n)$: the final output produced from sequentially executing every
input in $T$ starting in state $s$.
