\section{Evaluation}\seclabel{Evaluation}
In this section, we answer two questions: (1) How difficult is it to write
\watprovenance{} specifications? and (2) How do \watprovenance{} and
\watprovenance{} specifications compare to other debugging techniques?
We answer question $1$ in \secref{EvalWatProvSpecs} and question $2$ in
\secref{EvalWatProvRedis}.

\subsection{\WatProvenance{} Specifications}\seclabel{EvalWatProvSpecs}
\begin{table*}[t]
  \caption{\fluent{} \watprovenance{} specifications}
  \tablabel{ProvSpecLengths}
  \begin{tabular}{lllll}
    \toprule
    System    & Language & LOC & Number of APIs & Supported API                                                         \\\midrule
    Redis     & SQL      & 30  & 9              & get, set, del, append, incr, decr, incrby, decrby, strlen             \\
    POSIX     & Python   & 88  & 2              & reading and writing byte ranges                                       \\
    Amazon S3 & Python   & 200 & 5              & creating, copying, catting, removing, and listing objects and buckets \\
    Zookeeper & SQL      & 70  & 4              & creating, reading, writing, and listing znodes                        \\
    \bottomrule
  \end{tabular}
\end{table*}

In \secref{WatProvSpecs}, we argued that many commonly used distributed system
components have relatively simple \watprovenance{} specifications. In this
subsection, we substantiate our argument with concrete \watprovenance{}
specifications for Redis, a subset of the POSIX file system API, Amazon S3, and
Zookeeper.
%
We implemented these \watprovenance{} specifications using \fluent{}.
\tabref{ProvSpecLengths} lists the language in which we wrote each provenance
specification, the lines of code required to write each specification, the
number of APIs supported by each specification, and the API supported by each
specification.

We found it simple to write \watprovenance{} specifications for 17 of the 20
APIs. We found specifying three of the APIs slightly more challenging, but
still relatively straightforward.
%
First, specifying the read of a byte range in a file system required us to find
the most recent write to each segment of the byte range. This required us to
scan backwards through the trace, maintaining a disjoint set of byte ranges.
%
Second, specifying the catting and listing of objects in Amazon S3 was
complicated by the fact that objects could be moved across buckets. Thus,
computing the \watprovenance{} required computing the transitive
\watprovenance{} of objects that have been moved and copied.

As we discussed in \secref{WatProvSpecsLimitations}, this does not mean that
\emph{all} black boxes have simple \watprovenance{} specifications, but it
corroborates the claim that many commonly used black boxes do.

\subsection{Debugging with \WatProvenance}\seclabel{EvalWatProvRedis}
How do \watprovenance{} and \watprovenance{} specifications compare to other
debugging techniques? We answer this question by comparing \fluent{}, our
prototype \watprovenance{} debugger, against two other debugging techniques:
SPADE~\cite{gehani2012spade} and \texttt{printf} debugging. We qualitatively
evaluate the three debugging techniques using four metrics---ease of adoption,
runtime overhead, high-level debugging support, and low-level debugging
support---which we will explain momentarily. This qualitative analysis is
summarized in \tabref{DebugComparison}.

\begin{table*}[t]
  \caption{A qualitative comparison of debugging techniques}
  \tablabel{DebugComparison}
  \begin{tabular}{lllp{3cm}p{3cm}}
    \toprule
    Debugging Technique       & Ease Of Adoption & Runtime Overheads & Supports High-Level Debugging & Supports Low-Level Debugging \\\midrule
    SPADE (Audit Logs)        & easy             & low               & no                            & some \\
    SPADE (LLVM)              & easy             & high              & no                            & yes \\
    \texttt{printf} Debugging & easy--impossible & low               & some                          & yes \\
    \fluent{}                 & easy--hard       & medium            & yes                           & no \\
    % GDB                     & easy             & low               & hard                          & easy \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsubsection{SPADE}
SPADE~\cite{gehani2012spade} is a framework that collects provenance
information from arbitrary black boxes in a distributed system. SPADE collects
provenance information from a variety of sources including operating system
audit logs, network artifacts, LLVM instrumented applications, and applications
dynamically instrumented for taint analysis. These techniques are the current
state of the art in extracting the provenance of unmodified black boxes. For the
sake of brevity, we will focus only on the provenance that SPADE collects from
operating system audit logs and LLVM instrumented applications.

\paragraph{Ease of Adoption}
Our first metric, \defword{ease of adoption}, is a measure of how difficult it
is for a developer to set up the infrastructure required to use a particular
debugging technique. Ease-of-adoption does not include the difficulty of
actually debugging using a debugging technique; it only includes the difficulty
of making a system amenable to the debugging technique.
%
The ease of adoption of SPADE, for example, is very low. For SPADE to collect
operating system audit logs, binaries can be run unmodified. To collect LLVM
call graphs, binaries have to be compiled with LLVM instrumentation, but the
code itself remains unchanged.

% Note to self. We ran 100,000 single-threaded synchronous redis ping
% operations on an EC2 m5.xlarge. When run against a normal redis-server, it
% takes 2.45 seconds. When run against a LLVM-traced server, it takes 29.766
% seconds. Both the client and server were run on the same machine. The
% provenance was not even being stored.
\paragraph{Runtime Overhead}
Our second metric, \defword{runtime overhead}, is a measure of the performance
overheads that a debugging technique imposes on a system.
%
When SPADE collects operating system audit logs, it imposes a negligible amount
of runtime overhead, as operating system audit logs are created whether or not
SPADE is being used. LLVM instrumented binaries, on the other hand, run
significantly slower than their uninstrumented counterparts. As a simple
example, on an Amazon EC2 m5.xlarge instance, we measured that a single Redis
client required 2.45 seconds to send 100,000 synchronous \texttt{PING}
operations to a default configured Redis server running on the same machine.
When we compiled the Redis server with LLVM instrumentation, the client
required 29.76 seconds, an order of magnitude decrease in throughput.

\paragraph{High- and Low-Level Debugging Support}
Our third and fourth metrics, \defword{high-level debugging support} and
\defword{low-level debugging support}, are measures of whether a debugging
technique facilitates high-level and low-level debugging. As we described in
\secref{WatProvLimitations}, high-level debugging involves understanding which
events in a distributed system cause each other, how events are ordered with
respect to one another, etc. Conversely, low-level debugging involves reasoning
about the details of how a particular program executes.

We ran SPADE---with audit logging and LLVM instrumentation enabled---against a
trivial workload consisting of a single set and get to Redis. SPADE produced
1,087 audit log provenance entries and 1,118,764 LLVM instrumentation
provenance entries. High-level debugging with either of these sources of
provenance is difficult due to the provenance's size and detailed nature.
Understanding the reported provenance requires either an understanding of how
particular syscalls relate to Redis' source code (for audit logs) or a detailed
understanding of Redis' implementation (for LLVM instrumentation). Low-level
debugging with audit logs is challenging, because the audit logs lack
information about the execution of the code being debugged (besides the
syscalls that it makes). Low-level debugging with LLVM instrumentation is easier
but still challenging due to the sheer volume of provenance information
produced.

\newcommand{\printf}{\texttt{printf}}
\subsubsection{\printf{} Debugging}
``\printf{} debugging'' is the ad-hoc debugging technique in which a developer
adds \printf{} statements (or log statements) to the various components of a
distributed system and then debugs the system by analyzing the resulting logs.

\paragraph{Ease of Adoption}
The ease of implementing \printf{} debugging depends on (a) how many log
statements a developer wants to add and (b) the piece of code to which a
developer wants to add log statements. It is relatively easy for a developer to
add logging to a piece of code that they wrote because they know where and what
to log. If the code is a complex piece of open source software (e.g. Apache
Cassandra, Apache Zookeeper), then adding log statements requires understanding
at least part of the software's implementation. If the code is closed-source
(e.g. Cloud Spanner, Amazon Redshift), then adding log statements is
impossible.

\paragraph{Runtime Overheads}
The overheads of logging are typically negligible compared to heavier weight
techniques like LLVM instrumentation or \fluent{} shims.

\paragraph{High- and Low-Level Debugging Support}
The ability to perform high- and low-level debugging with log statements is
challenging to characterize. It varies from easy to difficult based on the
complexity of the bug being debugged, the quantity and quality of logs, etc.
Still, for high-level debugging, there is an unavoidable burden imposed by
\printf{} debugging. Developers have to make sure enough information is logged,
collect logs from a number of machines (some of which might have crashed),
filter out irrelevant entries from the potentially voluminous logs, and trace
data across multiple logs.

\subsubsection{\fluent{}}
\paragraph{Ease of Adoption}
To adopt \fluent{}, a developer must first write \watprovenance{}
specifications for each component in the system. As we discussed, many
distributed system components have relatively simple \watprovenance{}
specifications for which adopting Watermelon is relatively easy, while some
components have very complex \watprovenance{} specifications that make
\fluent{} more challenging to adopt.

\paragraph{Runtime Overheads}
\fluent{} shims introduce a non-negligible amount of runtime overhead by
intercepting network messages and periodically persisting traces. We measured
the performance overheads of \fluent{} shims on trivial workload in which a
Redis client performs a series of \texttt{SET} operations against a Redis
server. \fluent{} shims decreased the system's throughput by 41.4\%. While some
of this performance degradation is an artifact of our prototype, some is
inherent to shims.

\paragraph{High- and Low-Level Debugging Support}
\fluent{}'s greatest strength is that it makes high-level debugging easy.
\Watprovenance{} formalizes the intuitive notion of which inputs in a
distributed system cause a particular output. \Watprovenance{} specifications
allow a developer to codify these intuitions and automatically use them to
filter irrelevant information when debugging. By refining causality,
\watprovenance{} also allows developers to reason about the ordering of events
in a distributed system without having to manually trace events through a set
of logs. Conversely, \fluent{} must be paired with another system to
support low-level debugging.

As a simple experiment, we again ran our trivial workload consisting of a
single set and get to Redis. Whereas SPADE produced 1,089 audit log provenance
entries and 1,118,764 LLVM instrumentation provenance entries, \fluent{}
produced 8 provenance entries: the get and set request and response recorded on
the client and the server.
