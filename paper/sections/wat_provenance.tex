\section{\WatProvenance{}}\seclabel{WatProvenance}

\subsection{A Motivating Example}\seclabel{WatProvenanceExample}
\newcommand{\systemname}{ZardozBook}
We claim that there is currently no \emph{precise} way to reason about the
causes of events in a heterogenous distributed system. To best understand why,
let's look at a simple example.  Consider the implementation of a Facebook-like
social media application called \systemname{}, illustrated in
\figref{MeanComment}. \systemname{} users can post status updates to their
wall, and these updates are only viewable by their friends on the site. Users
send requests to a load balancer that forwards the requests to one of three
weakly consistent Redis-backed application servers: $s_1$, $s_2$, and $s_3$.
These application servers store a cache of \systemname{}'s data in Redis and
periodically synchronize their caches with a centralized Postgres database.

Consider a scenario in which Ava, a \systemname{} user, has a falling out with
Bob, a friend of hers on the site. Ava unfriends Bob and then posts the status
``Bob is a big jerk!'' to her wall, thinking that Bob will not see the status
because he is no longer her \systemname{} friend. Unfortunately, Bob later logs
in to \systemname{} and sees Ava's mean comment!

{\input{figures/mean_comment}}

Why did this happen? Here's an informal account. Ava's request to unfriend Bob
was forwarded to application server $s_1$ by the load balancer.  Then, Ava's
request to post the mean comment about Bob was forwarded to $s_2$.  $s_2$ then
pushed the comment to the Postgres repository.  $s_3$ then issued a SQL query
to the Postgres repository, pulling the latest data into its Redis cache. In
doing so, it pulled in Ava's mean comment.  Finally, when Bob logged in, his
request was forwarded to $s_3$ which returned the mean comment.

We argue that there is currently no \emph{systematic} way to go about
discovering this sequence of events. One possibility is to use causality, as
described in \secref{Background}. We could instrument our distributed system to
record the causal history of every event that takes place in the system. Then,
we could examine the causal history of Bob's request in an attempt to diagnose
why Bob was seeing Ava's mean comment. Unfortunately, this is not helpful. The
causal history of Bob's request includes \emph{every} event that causally
precedes it, whether or not the event is actually relevant. For example, the
causal history of Bob's request would include every single message that was
received by server $s_3$ prior to Bob sending his request, even those that do
not involve Ava and Bob. The problem is that causality is too coarse-grained.
It fails to incorporate any notion of a system's semantics as a means to filter
out irrelevant messages.  Instead, it returns a vast overapproximation of all
the events that \emph{might} cause an event instead of the events that
\emph{actually do} cause an event.

This might prompt us to try and apply ideas from data provenance. Unlike
causality, \whyprovenance{} does incorporate system semantics in order to
return the actual causes of a particular output of a query. As discussed
above, \whyprovenance{} has two fatal flaws.  First is \whyprovenance{}'s
restriction to relational queries: while we might be able to use
\whyprovenance{} to debug $s_3$'s SQL query that was sent to the Postgres
database, that is only one small piece of the puzzle.  Understanding why Bob
saw Ava's mean comment requires us to reason about messages that travel through
our application servers, our redis servers, and our load balancer. But, these
are not relational databases, so we cannot apply \whyprovenance{} to them.
Second and more fundamental is \whyprovenance{}'s inability to reflect any
notion of state change over time.  In particular, the real ``why'' question we
want to answer here is ``why was the provenance of Bob's query unaffected by
Ava's unfriend request?'' The \whyprovenance{} of Bob's query has no answer to
this question: it knows nothing about updates or the order in which they
happen.

\subsection{\WatProvenance{}}
In isolation, causality and data provenance are both insufficient to diagnose
why Bob saw Ava's mean comment. Understanding the root cause of this anomalous
behavior requires us to reason about the ordering of events within the network
(as with causality) \emph{and} the precise data dependencies between different
requests (as with data provenance).
%
\defword{\Watprovenance{}} is a novel form of data provenance that unifies the
benefits of causality and data provenance. Borrowing from causality,
\watprovenance{} is a general-purpose mechanism that can be applied to
arbitrary state machines. Borrowing from \whyprovenance{}, \watprovenance{}
incorporates system semantics to produce the actual causes of a particular
event, rather than a conservative overapproximation.

More formally, we consider a state machine $M = (S, s_0, \Sigma, \Lambda,
\delta, \epsilon)$, a trace $T \in \Sigma^*$, and a particular input $i \in
\Sigma$. The state machine begins in state $s_0$ and executes $T$,
transitioning to state $s_T = \delta^*(s_0, T)$. It then executes input $i$
producing output $o = \epsilon^*(s_0, Ti) = \epsilon(s_T, i)$. \Watprovenance{}
aims to formalize an intuitive notion of \emph{why} the state machine $M$
produces output $o$ when given input $i$. We build this intuitive notion by way
of a sequence of examples, ultimately leading to the definition of
\watprovenance{}.

\begin{example}\examplelabel{WatExampleKvs}
  \newcommand{\kvset}{\text{set}}
  \newcommand{\kvget}{\text{get}}
  Consider a key-value server state machine $M$ with an input alphabet that
  consists of sets and gets to integer-valued variables. Consider the trace
  \[
    T = \kvset(x, 1); \kvset(y, 2)
  \]
  that consists of two requests: one that sets the value of $x$ to $1$ and one
  that sets the value of $y$ to $2$. Consider request $i = \kvget(x)$ which
  requests the value of $x$. When $M$ processes trace $T$ and then input $i$,
  it expectedly outputs $1$ (i.e.\ $o = \epsilon^*(s_0, Ti) = 1$).

  Why did $M$ output $1$? Well, in this trivial example, the reason is
  abundantly clear. $M$ returned $1$ as the value of $x$ because the first
  request in $T$ set $x$'s value to $1$. More formally, the subtrace $T' =
  \kvset(x, 1)$ of $T$ suffices to generate the output of $1$ (i.e.
  $\epsilon^*(s_0, T'i) = o$). The lesson here is that \textbf{the cause of an
  output $o$ is a subtrace of the input that is sufficient to generate $o$}. We
  call such a subtrace a \defword{witness} of $o$.

  However, the entire trace $T$ is also a witness. That is, $T$ also suffices
  to generate an output of $1$. But, the $\kvset(y, 2)$ request is not relevant
  to our $\kvget(x)$ request, so we shouldn't include it as a cause of our
  output. Thus, we revise our earlier observation; \textbf{the cause of an
  output $o$ is a subtrace of the input that is sufficient to generate $o$ and
  is also in some sense minimal}. We will define minimality more carefully
  below.
\end{example}

\begin{example}\examplelabel{WatExampleFormulas}
  \newcommand{\Mset}{\text{set}}
  \newcommand{\Meval}{\text{eval}}
  Consider a state machine $M$ that stores a set of boolean-valued variables.
  Users can set variables to true or false and can request that $M$ evaluate
  formulas over these variables. For example, consider the trace
  \[
    T = \Mset(a); \Mset(b); \Mset(c); \Mset(d)
  \]
  that sets variables $a, b, c, d$ to true. Further, consider the input $i =
  \Meval((a \land d) \lor (b \land c))$ that requests $M$ evaluate the formula
  $(a \land d) \lor (b \land c)$. $o = \epsilon^*(s_0, Ti)$ is expectedly true;
  $a, b, c, d$ are all true, so the formula evaluates to true.

  Why did $M$ output true? Well, there are two reasons. The first is the
  subtrace $T_{ad} = \Mset(a); \Mset(d)$, and the second is the subtrace
  $T_{bc} = \Mset(b); \Mset(c)$. Both of these subtraces are witnesses of $o$,
  so we should include both in an explanation of our output. We again revise
  the lesson from our previous example; \textbf{the cause of an output $o$ is a
  \emph{set} of witnesses of $o$}.
\end{example}

\begin{example}\examplelabel{WatExampleAnotbc}
  \newcommand{\Mset}{\text{set}}
  \newcommand{\Meval}{\text{eval}}
  Consider again the state machine $M$ from the previous example, and consider
  the trace
  \[
    T = \Mset(a); \Mset(b); \Mset(c)
  \]
  and request $i = \Meval((a \land \lnot b) \lor c)$. $o = \epsilon^*(s_0, Ti)$
  is true. $a \land \lnot b$ evaluates to false because $\lnot b$ is false, but
  $c$ is true, so $(a \land \lnot b) \lor c$ is true.
  %
  Why did $M$ output true? Well, as we just explained $(a \land \lnot b) \lor
  c$ is true solely because $c$ is true. Thus, the subtrace $T_c = \Mset(c)$
  should be the only explanation. However, the subtrace $T_a = \Mset(a)$ is
  also a witness! If $M$ executes $T_a$ and then $i$, $M$ will output true.

  This is certainly not what we want. $\Mset(a)$ does not contribute to our
  output, so it should be excluded. The problem here is that the subtrace $T_a$
  does not include the $\Mset(b)$ request that ultimately keeps the $\Mset(a)$
  request from satisfying the formula. From this, we see that in order for a
  witness $T'$ to be a good explanation of a particular output $o$, \textbf{it
    must be that every supertrace of $T'$ in $T$ is also a witness of $o$}.
    $T_{ab} = \Mset(a); \Mset(b)$ is a supertrace of $T_a$ in $T$, but $T_{ab}$
    does not suffice to generate $o$. Thus, $a$ is not a valid witness.
\end{example}

Combining our lessons from \exampleref{WatExampleKvs},
\exampleref{WatExampleFormulas}, and \exampleref{WatExampleAnotbc}, we arrive
at our definition of \watprovenance{}. Given a state machine $M$, an input
trace $T$, an input $i$, and the corresponding output $o = \epsilon^*(s_0,
Ti)$, we say that a subtrace $T'$ of $T$ is a \defword{witness} of $o$ if
$\epsilon^*(s_0, T'i) = o$. We say that a witness $T'$ of $o$ is
\defword{closed under supertrace in $T$} if every supertrace of $T'$ in $T$ is
also a witness of $o$. Let $\Wit{M, T, i}$ be the set of witnesses of $o$ that
are closed under supertrace in $T$. The \defword{\watprovenance{}} of input $i$
with respect to $M$ and $T$, abbreviated $\Wat{M, T, i}$, is the set of minimal
elements of $\Wit{M, T, i}$. That is, $\Wat{M, T, i}$ consists of every witness
$T'$ of $o$ such that (1) $T'$ is closed under supertrace in $T$, and (2) no
proper subtrace of $T'$ is also a witness of $o$ that satisfies (1)\footnote{%
  Note the subtlety that in order to compute $\Wat{M, T, i}$, we first compute
  all the witnesses of $o$ that are closed under supertrace and then remove the
  non-minimal elements. We do \emph{not} compute all the minimal witnesses and
  then remove the ones that are not closed under supertrace in $T$.
  Informally, we compute minimal(closed\_under\_supertrace(witnesses)), not
  closed\_under\_supertrace(minimal(witnesses)).
  These two are not the same. See \exampleref{WatExampleAdds}, for example.
}. Note that we formally define \watprovenance{} with respect to an input $i$,
but colloquially discuss \watprovenance{} with respect to the corresponding
output $o$.

\subsection{Can I Get a Witness?}
We now provide a few more simple examples of \watprovenance{} to illustrate
the definition. In \secref{WatProvSpecs}, we describe the \watprovenance{} of
realistic servers such as Redis, Zookeeper, and S3.

\begin{example}\examplelabel{WatExampleXyx}
  \newcommand{\Mget}{\text{get}}
  \newcommand{\Mset}{\text{set}}
  Consider again the key-value server state machine from
  \exampleref{WatExampleKvs}, the trace
  \[
    T = a_1 a_2 a_3 = \Mset(x, 1); \Mset(x, 2); \Mset(x, 1)
  \]
  and the input $i = \Mget(x)$. $o = \epsilon^*(s_0, Ti) = 1$. To compute
  $\Wat{M, T, i}$ (the \watprovenance{} of $o$), we first compute $\Wit{M, T,
  i}$ (the witnesses of $o$ that are closed under supertrace in $T$).


  $T_3 = a_3 = \Mset(x, 1)$ suffices to generate $o$ and is closed under
  supertrace in $T$ because the proper supertraces $a_1a_3$, $a_2a_3$, and
  $a_1a_2a_3$ all generate $o$.  So, $T_3 \in \Wit{M, T, i}$. By a similar line
  of reasoning, we also find that $a_1a_3$, $a_2a_3$, and $a_1a_2a_3$ are in
  $\Wit{M, T, i}$.

  The subtrace $T_1 = a_1 = \Mset(x, 1)$ is also a witness of $o$, but it is
  not closed under supertrace in $T$ because the supertrace $a_1a_2 = \Mset(x,
  1); \Mset(x, 2)$ does not generate $o$. Thus, $T_1$ is not in $\Wit{M, T, i}$
  and therefore not in $\Wat{M, T, i}$.  Intuitively this is correct because it
  is the second $\Mset(x, 1)$ command, not the first, that causes the value of
  $x$ to ultimately be $1$.

  All other subtraces of $T$ are not witnesses of $o$, so $\Wit{M, T, i} =
  \set{a_3, a_1a_3, a_2a_3, a_1a_2a_3}$ which has unique minimal element $a_3$.
  Thus, $\Wat{M, T, i} = \set{a_3}$.
\end{example}

\begin{example}\examplelabel{WatExampleAdds}
  \newcommand{\Mget}{\text{get}}
  \newcommand{\Mset}{\text{set}}
  \newcommand{\Madd}{\text{add}}
  \newcommand{\Msub}{\text{sub}}
  Consider again the key-value server state machine from
  \exampleref{WatExampleKvs} and \exampleref{WatExampleXyx} with the input
  alphabet expanded to include additions and subtractions to a particular
  variable. Consider the trace
  \[
    T = a_1 a_2 a_3 a_4 = \Mset(x, 42); \Madd(x, 1); \Madd(x, 2); \Msub(x, 3)
  \]
  and the request $i = \Mget(x)$. $o = \epsilon^*(s_0, Ti) = 42$.

  Again, we compute $\Wit{M, T, i}$. $T$ suffices to generate $o$, and $T$
  does not have any proper supertraces in $T$, so it is trivially closed under
  supertrace in $T$. Thus, $T \in \Wit{M, T, i}$.
  %
  $T_1 = a_1 = \Mset(x, 42)$ is the only other witness of $o$, but $T_1$ is not
  closed under supertrace in $T$. $T_{12} = \Mset(x, 42); \Madd(x, 1)$ is a
  supertrace of $T_1$ but does not generate $o$.
  %
  Thus, $\Wit{M, T, i} = \Wat{M, T, i} = \set{T}$.

  Note that we first compute the witnesses that are closed under supertrace in
  $T$ and then remove the non-minimal elements. Imagine instead if we had first
  computed the minimal witnesses and then removed the elements that were not
  closed under supertrace in $T$. We would have found that the sole minimal
  witness of $o$ was $T_1$. Then, we would have filtered out $T_1$ because, as
  we just saw, it is not closed under supertrace in $T$. This would leave us
  with an empty \watprovenance{}!
\end{example}

\begin{example}\examplelabel{WatExampleSetDiff}
  \newcommand{\Mins}{\text{insert}}
  \newcommand{\Mquery}{\text{query}}
  Consider a relational database state machine $M$. The input alphabet of $M$
  includes commands to insert a tuple into $M$ and to execute a relational
  algebra query against $M$. Consider the trace
  \[
    T = a_1 a_2 a_3 = \Mins(R, t); \Mins(R, u); \Mins(S, u)
  \]
  that inserts tuple $t$ into relation $R$, inserts tuple $u$ into relation
  $R$, and inserts tuple $u$ into relation $S$. Consider the request $i =
  \Mquery(R - S)$ that queries the set difference $R - S$ of $R$ and $S$. $o =
  \epsilon^*(s_0, Ti) = \set{t}$ is the set of only the tuple $t$.

  We first compute $\Wit{M, T, i}$. There are only three witnesses of $o$:
  $a_1$, $a_1a_3$, and $a_1a_2a_3$.  $a_1$ is not closed under supertrace in
  $T$ because the supertrace $a_1a_2$ does not generate $o$. The other two
  traces, $a_1a_3$ and $a_1a_2a_3$, are closed under supertrace. Thus, $\Wit{M,
  T, i} = \set{a_1a_3, a_1a_2a_3}$, and $\Wat{M, T, i} = \set{a_1a_3}$.
\end{example}

\subsection{\WatProvenance{} Properties}
\exampleref{WatExampleKvs} through \exampleref{WatExampleSetDiff} demonstrate
that our definition of \watprovenance{} accurately models how we intuitively
think about data provenance for state machines.  We now discuss some additional
properties about \watprovenance{}. In particular, we discuss how
\watprovenance{} provenance relates to \whyprovenance{} and causality.

\begin{claim}\clmlabel{Existence}
  For every state machine $M$, trace $T$, and input $i$, $\Wat{M, T, i}$ is
  non-empty.
\end{claim}

\clmref{Existence} is a simple sanity check for our definition of
\watprovenance{}. It would be odd if something had no provenance at all! The
proof is trivial. $T$ is always a witness of $o$ that is closed under
supertrace in $T$. Thus, $\Wit{M, T, i}$ (and hence $\Wat{M, T, i}$) is
non-empty.

\newcommand{\Min}{\text{in}}
\begin{claim}\clmlabel{WatSubsumesWhy}
  Let $M$ be a general relational database state machine (first introduced in
  \exampleref{WatExampleKvs}) that allows for the insertions of tuples and the
  execution of monotone relational queries (i.e. queries composed of the
  monotone relational algebra operators select, project, join and union). Let
  $I$ be an arbitrary database instance, and let $T$ be a trace which inserts
  every tuple in $I$. Let input $i = \Min(t, Q)$ be an input which returns a
  boolean that indicates whether tuple $t$ is in $Q(I)$.  Then, viewing a
  subtrace $T'$ as a subinstance $I' \subseteq I$, $\Wat{M, T, i} = \MWhy{Q, I,
  t}$.
\end{claim}

In short, \clmref{WatSubsumesWhy} says that \watprovenance{} subsumes
minimal \whyprovenance{}. \Watprovenance{} is defined for an arbitrary state
machine, and selecting a relational database state machine, we see that minimal
\whyprovenance{} is just a special case of \watprovenance{}.

The proof is straightforward.
%
If $t \notin Q(I)$, then $o = \epsilon^*(s_0, Ti)$ returns false and $\Wat{Q,
I, t}$ consists only of the empty trace, indicating that $\MWhy{Q, I, t}$ is
empty.
%
Otherwise $t \in Q(I)$ and $o$ returns true. Consider a witness $T' \in \Wat{M,
T, i}$. $T'$ suffices to generate $o$, so the corresponding instance $I'$
suffices to generate $t$. Moreover, because $Q$ is monotone and $T$ does not
contain any deletions, every witness $T'$ is closed under supertrace in $T$.
Thus, $T'$ (and hence $I'$) is a minimal witness. The proof of the
converse---showing that every $I' \in \MWhy{Q, I, t}$ has a corresponding
subtrace $T' \in \Wat{M, I, t}$---is almost identical.

While \watprovenance{} \emph{generalizes} \whyprovenance{}, it \emph{refines}
causality in the following sense. Consider a state machine $M$ that executes a
trace $T$ and then an input request $i$. The causal history of $i$ includes
every single request in $T$ (and the causal history of every request in $T$)
whether or not the request in $T$ actually did influence the output of
executing request $i$. \Watprovenance{} instead returns the subtraces of $T$
that are actual causes. Note that \watprovenance{} returns a set of witnesses
that are local to a particular node. On its own, \watprovenance{} does not
return the causal history of these witnesses, which includes messages sent by
other nodes in the distributed system. In \secref{Fluent}, we see how to enrich
\watprovenance{} with this information.
